# üìö yu-rag-papers

A **Retrieval‚ÄëAugmented Generation (RAG)** playground that wires up FastAPI, PostgreSQL, FAISS vector storage, an embedding worker, and a LINE Bot into a single reproducible environment.

*Built for quick experiments, demos, and personal projects.*

---

## ‚ú® What's inside?

| Layer        | Tech (‰∏ªË¶ÅÂ•ó‰ª∂ / Êò†ÂÉè)                                          | Purpose / Role                                                          |
| -------------| -------------------------------------------------------------- | ----------------------------------------------------------------------- |
| **API**      | FastAPI + Uvicorn <br>LangChain (RAG Orchestration)           | REST / Webhook endpoints, stitches **Retriever ‚Üí LLM** into chat flow   |
| **Vector DB**| FAISS (index-flat/IP)                                         | Stores document chunks & dense vectors for semantic search              |
| **Embedding**| Worker container (`docker/Dockerfile.embed`) <br>Hugging Face Transformers + LangChain Embeddings | Offline jobÔºöchunk files ‚Üí generate embeddings ‚Üí write to Vector DB     |
| **LLM**      | TinyLlama-1.1B-Chat (GGUF) <br>llama-cpp-python / ctransformers | Local inference for answer generation (keeps data on-prem)             |
| **Bot**      | LINE Messaging API (SDK)                                       | Chat interface for end users; forwards queries to API                   |
| **Tunnel**   | ngrok                                                          | Expose local API to LINE for webhook callbacks during dev               |

> üê≥ **Everything is containerised** ‚Äì simply run `docker compose up` and you'll have the database, embedding worker, local LLM and API + LINE webhook all wired together.

---

## üîç Key Features

- **Academic Paper RAG**: The system can ingest arXiv papers and make them searchable (currently loaded with papers about transformers in NLP)
- **Fully Dockerized**: All components run in containers for easy deployment and testing
- **FAISS Vector Storage**: Efficient similarity search with FAISS vector indices
- **Local LLM Support**: Uses TinyLlama for inference, keeping all data local
- **Conversation Memory**: Maintains chat history in PostgreSQL database per user
- **LINE Bot Integration**: Ready-to-use LINE messaging interface with automated webhook setup
- **Production-Ready Architecture**: Follows best practices with multi-stage Docker builds and component isolation

---

## üóÇ Project layout

```text
‚îú‚îÄ‚îÄ app/                          # FastAPI ÊúçÂãô ‚îÄ Web Layer
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # ÂÖ•Âè£ÔºöÊéõËºâ router„ÄÅ‰∫ã‰ª∂„ÄÅCORS
‚îÇ   ‚îÇ                             # ‚ûú ‰∫¶Áµ±‰∏ÄÊ≥®ÂÖ• settings„ÄÅÂïüÂãïÔºèÈóúÈñâ‰∫ã‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ routes/                   # REST / Webhook Ë∑ØÁî±
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ line.py               # LINE Bot Webhook + È©óË≠âÁ∞ΩÂêç
‚îÇ   ‚îî‚îÄ‚îÄ db/                       # Ë≥áÊñôÂ∫´Áõ∏ÈóúÂ∞ÅË£ù
‚îÇ       ‚îú‚îÄ‚îÄ database.py           # Âª∫Á´ã SQLAlchemy Engine & SessionLocal
‚îÇ       ‚îú‚îÄ‚îÄ memory.py             # ‚ûú Âø´ÂèñÊúÄËøëÂ∞çË©±ÔºåÁî®Êñº RAG Memory
‚îÇ       ‚îî‚îÄ‚îÄ models.py             # ORM ModelsÔºàUser & ChatHistoryÔºâ
‚îÇ
‚îú‚îÄ‚îÄ crawler/                      # Á∂≤Á´ôÁà¨Ëü≤ & ÂÆöÊôÇÊéíÁ®ã
‚îÇ   ‚îú‚îÄ‚îÄ fetcher.py                # ÊäìÊåáÂÆöÁ∂≤ÂùÄ /S3 /RSSÔºåËêΩÂú∞Âà∞ data/pdf
‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py              # APScheduler ‰ªªÂãôÔºåÂÆöÊôÇÂëºÂè´ fetcher
‚îÇ
‚îú‚îÄ‚îÄ rag/                          # Retrieval-Augmented Generation Ê†∏ÂøÉ
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py             # ËÆÄÂèñ sentence-transformers or local embedding
‚îÇ   ‚îú‚îÄ‚îÄ rag_chain.py              # Âª∫Á´ã LLMChainÔºàRetriever + LLM + MemoryÔºâ
‚îÇ   ‚îî‚îÄ‚îÄ fine_tune.py              # ‚ûú LoRA / PEFT ÂæÆË™øËÖ≥Êú¨ÔºàÂèØÈÅ∏Áî®Ôºâ
‚îÇ
‚îú‚îÄ‚îÄ data/                         # ‰ΩøÁî®ËÄÖËàáÁà¨Ëü≤Ë≥áÊñô„ÄÅÂêëÈáèÂ∫´ÔºàÊåÅ‰πÖÂåñÔºâ
‚îÇ   ‚îú‚îÄ‚îÄ faiss/                    # FAISS index Ôºõembed ÂÆπÂô®ÊúÉÂØ´ÂÖ•
‚îÇ   ‚îú‚îÄ‚îÄ pdf/                      # ÂéüÂßã PDF
‚îÇ   ‚îî‚îÄ‚îÄ pdf_text/                 # Ëß£ÊûêÂæåÁöÑÁ¥îÊñáÂ≠óÔºàchunk ÂâçÔºâ
‚îÇ
‚îú‚îÄ‚îÄ models/                       # Êú¨Âú∞ LLM / EmbeddingÔºàgguf„ÄÅbinÔºâ
‚îÇ   ‚îî‚îÄ‚îÄ tinyllama-q4_K_M.gguf     # Á§∫‰æãÔºõÂØ¶ÈöõË∑ØÂæë‰æùËá™Ë°åÊéõËºâ
‚îÇ
‚îú‚îÄ‚îÄ requirements/                 # ‰æùË≥¥ÊãÜÂàÜÔºåÊ∏õÂ∞ëÈáçÂª∫ÊôÇÈñì
‚îÇ   ‚îú‚îÄ‚îÄ requirements.core.txt     # langchain, pydantic, tqdm...
‚îÇ   ‚îú‚îÄ‚îÄ requirements.api.txt      # fastapi, uvicorn[standard], psycopg2...
‚îÇ   ‚îî‚îÄ‚îÄ requirements.embed.txt    # sentence-transformers, faiss-cpu...
‚îÇ
‚îú‚îÄ‚îÄ docker/                       # Â§öÈöéÊÆµÊßãÂª∫ËÖ≥Êú¨
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.api            # FastAPI + ngrok + tinyllama runtime
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile.embed          # ÂµåÂÖ•ÊâπÊ¨°ÔºöÂÉÖÂÆâË£ùÊ†∏ÂøÉ & embedding
‚îÇ
‚îú‚îÄ‚îÄ bin/                          # ÂÆπÂô®ÂÖßÂïüÂãïËÖ≥Êú¨ / Êú¨Ê©üËºîÂä©Êåá‰ª§
‚îÇ   ‚îú‚îÄ‚îÄ start_api.sh              # Ë®≠ÂÆö uvicorn workers„ÄÅexpose 8000
‚îÇ   ‚îú‚îÄ‚îÄ ingest.sh                 # Âø´ÈÄüÂ∞éÂÖ•‰∏¶ËôïÁêÜ‰∏ÄÁØáË´ñÊñá
‚îÇ   ‚îî‚îÄ‚îÄ dev_start_api.sh          # Áî®ÊñºÊú¨Âú∞ÈñãÁôºÁöÑAPIÂïüÂãïÊåá‰ª§
‚îÇ
‚îú‚îÄ‚îÄ .env.example                  # ÁØÑ‰æãÁí∞Â¢ÉËÆäÊï∏Ôºåcp ÁÇ∫ .env
‚îú‚îÄ‚îÄ docker-compose.yml            # db + embed + api ‰∏âÂÆπÂô®ÂçîË™ø
‚îî‚îÄ‚îÄ README.md                     # Â∞àÊ°àÊñá‰ª∂ÔºàÂÆâË£ù„ÄÅÂü∑Ë°å„ÄÅFAQÔºâ
```

---

## üìã Current Implementation

This project currently includes:

1. **Academic Paper RAG**: Four NLP papers from arXiv have been ingested:
   - 2004.11886v1.pdf
   - 2007.06257v2.pdf
   - 2106.02242v2.pdf
   - 2305.08800v1.pdf

2. **Embedding Pipeline**:
   - PDF fetching from arXiv via `crawler/fetcher.py`
   - Text extraction and paragraph segmentation
   - Vector generation using `sentence-transformers/all-MiniLM-L6-v2`
   - FAISS index storage for efficient retrieval

3. **Database Structure**:
   - User storage with LINE user IDs
   - Chat history with timestamps for conversation memory
   - Conversation persistence for contextual responses

4. **LLM Integration**:
   - TinyLlama local inference
   - Support for PEFT/LoRA fine-tuning

5. **LINE Bot**:
   - Automatic webhook setup using ngrok
   - Message validation and processing
   - Response formatting for LINE's character limits

---

## üöÄ Quick start (Docker Compose)

> **Prerequisites**  Docker Desktop 24+ or Docker Engine 20.10+.

1. **Clone & configure**

   ```bash
   git clone https://github.com/<yourname>/yu-rag-papers.git
   cd yu-rag-papers
   cp .env.example .env   # fill in the blanks
   ```

   | Variable                                            | Description                                                            |
   | --------------------------------------------------- | ---------------------------------------------------------------------- |
   | `OPENAI_API_KEY`                                    | Key for generating embeddings (or leave empty if you use local models) |
   | `NGROK_AUTHTOKEN`                                   | Your ngrok auth token (get one at ngrok.com)                           |
   | `LINE_CHANNEL_ACCESS_TOKEN` / `LINE_CHANNEL_SECRET` | Credentials for LINE Bot                                               |

2. **Launch everything**

   ```bash
   docker compose up --build
   ```

   ‚Äë API ‚Üí [http://localhost:8000](http://localhost:8000)
   ‚Äë Swagger ‚Üí [http://localhost:8000/docs](http://localhost:8000/docs)
   ‚Äë Postgres ‚Üí localhost:5432 (`rag_user` / `rag_pass`)

   The API container will print the autogenerated **Webhook URL** (ngrok) to register in the LINE console, e.g.

   ```text
   üîó  Webhook URL: https://xxxxx.ngrok-free.app/webhook/line
   ‚úÖ  LINE Bot ready!
   ```

3. **Tear down**

   ```bash
   docker compose down -v   # -v removes the named volume `db_data`
   ```

---

## üß™ Running locally without Docker

1. **Start Postgres** (native, Homebrew, or a tiny container):

   ```bash
   docker run -d --name pg -e POSTGRES_USER=rag_user -e POSTGRES_PASSWORD=rag_pass -e POSTGRES_DB=rag_db -p 5432:5432 postgres:16
   ```

2. **Create a virtualenv & install deps**

   ```bash
   python -m venv venv && source venv/bin/activate
   pip install -r requirements/requirements.core.txt -r requirements/requirements.api.txt
   ```

3. **Set env vars & run**

   ```bash
   export DATABASE_URL="postgresql+psycopg2://rag_user:rag_pass@localhost:5432/rag_db"
   export OPENAI_API_KEY=...
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```

   (Optional) run `ngrok http 8000` to expose the webhook.

---

## üîß Adding New Papers

To add new academic papers to the system:

```bash
# Quick import using bin/ingest.sh
source bin/ingest.sh

# Or manually run the crawler with custom parameters
python -m crawler.fetcher --query "cat:cs.CL AND transformers" --max 5
```

After adding papers, you need to generate embeddings:

```bash
# Generate embeddings for all PDFs in the data/pdf directory
python -m rag.embeddings
```

---

## üì± Testing the LINE Bot

After starting the system and seeing the webhook URL:

1. Go to the [LINE Developers Console](https://developers.line.biz/console/)
2. Navigate to your bot's settings
3. Update the webhook URL to match the one provided by the system
4. Enable webhook in the settings
5. Send a message to your bot and it should respond using RAG from your academic papers

---

## üß© Extending the System

This framework is designed to be modular. You can:

1. Replace the LLM by changing the model in `rag/rag_chain.py`
2. Add new document sources by implementing custom crawlers
3. Implement additional chat interfaces beyond LINE
4. Fine-tune the LLM using the provided scripts in `fine_tune/`

---

## üìÑ License

See the [LICENSE](LICENSE) file for details.
