"""
rag/rag_chain.py
================
æŠŠ FAISS å‘é‡è³‡æ–™åº«æ¥æˆ LangChain Conversational RAGï¼Œ
ä¸¦ä¿ç•™ä¸€å€‹ç°¡æ˜“ CLI æ–¹ä¾¿åœ¨æœ¬æ©Ÿæ¸¬è©¦ã€‚

å…ˆåŸ·è¡Œï¼š
    python -m rag.embeddings --max 3
ä»¥ç”¢ç”Ÿ data/faiss/index.faiss èˆ‡ metadata.pkl
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Optional

import typer
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_models import ChatLlamaCpp
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from peft import PeftModel
# ---------- å¸¸æ•¸ ----------
INDEX_DIR = Path("data/faiss")
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
K = 3

MODEL_PATH = "models/tinyllama-q4_K_M.gguf"  # TinyLLaMA-GGUF è·¯å¾‘


# ---------- å·¥å…·å‡½å¼ ----------
def build_retriever():
    if not (INDEX_DIR / "index.faiss").exists():
        raise RuntimeError(f"âŒ å‘é‡åº«ä¸å­˜åœ¨ï¼š{INDEX_DIR}")
    embedder = HuggingFaceEmbeddings(model_name=MODEL_NAME)
    vs = FAISS.load_local(
        str(INDEX_DIR),
        embedder,
        allow_dangerous_deserialization=True,  # ç‚ºäº†åœ¨ CI è¼‰å…¥ pickle
    )
    return vs.as_retriever(search_kwargs={"k": K})


def build_chain(
    memory: Optional[ConversationBufferMemory] = None,
):
    """
    å»ºç«‹ ConversationalRetrievalChain.

    Parameters
    ----------
    memory : ConversationBufferMemory | None, default None
        è‹¥å‚³å…¥è‡ªè¨‚è¨˜æ†¶é«”ï¼ˆä¾‹å¦‚å«è³‡æ–™åº«æ­·å²ï¼‰ï¼Œ
        ç›´æ¥æ›è¼‰ï¼›å¦å‰‡ä½¿ç”¨é è¨­çš„ In-RAM Bufferã€‚
    """
    llm = ChatLlamaCpp(
        model_path=MODEL_PATH,
        temperature=0.1,
        max_tokens=512,
        n_ctx=4096,
        n_threads=4,  # GitHub runner æœ‰ 2 vCPUï¼Œå¯è¦–æƒ…æ³èª¿æ•´
        verbose=False,
    )
    adapter_dir = Path("rag/lora_adapter")
    if adapter_dir.exists():
        llm.client = PeftModel.from_pretrained(llm.client, adapter_dir.as_posix())


    retriever = build_retriever()
    if memory is None:
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
        )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=False,
    )
    return chain


# ---------- Typer CLI ----------
cli = typer.Typer()


@cli.command()
def chat():
    """é€²å…¥äº’å‹•å¼ RAG å°è©±ï¼›è¼¸å…¥ exit é›¢é–‹ã€‚"""
    chain = build_chain()
    print("ğŸ¤–  RAG Chat å•Ÿå‹•ï¼Œè¼¸å…¥ 'exit' é›¢é–‹")
    while True:
        q = input("ä½ ï¼š ").strip()
        if q.lower() in {"exit", "quit"}:
            break
        print("AIï¼š", chain({"question": q})["answer"], "\n")


if __name__ == "__main__":
    cli()
