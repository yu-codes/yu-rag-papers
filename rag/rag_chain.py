"""
rag/rag_chain.py
================
æŠŠ FAISS å‘é‡è³‡æ–™åº«æ¥æˆ LangChain Conversational RAGï¼Œ
ä¸¦åŠ ä¸Šä¸€æ®µæœ€å°äº’å‹• CLIï¼Œæ–¹ä¾¿ç›´æ¥æ¸¬è©¦ã€‚

å…ˆç¢ºå®šå·²ç¶“åŸ·è¡Œéï¼š
    python -m rag.embeddings --max 3
ç”¢ç”Ÿ  data/faiss/index.faiss  èˆ‡ metadata.pkl
"""
from pathlib import Path
import os
import typer

# from ctransformers.langchain import ChatCTransformers  # æ–°å¢

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_openai import ChatOpenAI  # â†– æ–°è·¯å¾‘

from langchain_community.chat_models import ChatLlamaCpp  # â† æ–°

# ---------- å¸¸æ•¸ ----------
INDEX_DIR = Path("data/faiss")
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
K = 3

MODEL_PATH = "models/tinyllama-q4_K_M.gguf"  # å­˜æ”¾æ–¼ repo æˆ– CI ä¸‹è¼‰

# ---------- å·¥å…·å‡½å¼ ----------
def build_retriever():
    embedder = HuggingFaceEmbeddings(model_name=MODEL_NAME)
    vs = FAISS.load_local(
        str(INDEX_DIR),
        embedder,
    allow_dangerous_deserialization=True,   # â† æ–°å¢
)
    return vs.as_retriever(search_kwargs={"k": K})


"""def build_chain():

    llm = ChatOpenAI(
        model="gpt-3.5-turbo", temperature=0, api_key=os.getenv("OPENAI_API_KEY")
    )
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=build_retriever(),
        memory=ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        ),
        return_source_documents=True,
    )"""


def build_chain():
    llm = ChatLlamaCpp(
        model_path=MODEL_PATH,
        temperature=0.1,
        max_tokens=512,
        n_ctx=4096,  # è¦–æ¨¡å‹æ”¯æ´èª¿æ•´
        n_threads=4,  # GitHub runner = 2 vCPUï¼Œå¯è¨­ 2
        verbose=False,
    )
    retriever = build_retriever()
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=False,
    )
    

# ---------- Typer CLI ----------
cli = typer.Typer()


@cli.command()
def chat():
    """é€²å…¥äº’å‹•å¼ RAG å°è©±ï¼›è¼¸å…¥ exit é›¢é–‹ã€‚"""
    chain = build_chain()
    print("ğŸ¤– RAG Chat å•Ÿå‹•ï¼Œè¼¸å…¥ 'exit' é›¢é–‹")
    while True:
        q = input("ä½ ï¼š ").strip()
        if q.lower() in {"exit", "quit"}:
            break
        print("AIï¼š", chain({"question": q})["answer"], "\n")


if __name__ == "__main__":
    cli()
